{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "blessed-barbados",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "focal-upset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for URL patterns\n",
    "https://www.imdb.com/search/title/?title_type=tv_series&release_date=2012-01-01,2020-12-31&countries=us&languages=en&runtime=5,&view=simple&sort=release_date,asc&count=250&start=9501&ref_=adv_nxt\n",
    "https://www.imdb.com/search/title/?title_type=tv_series&release_date=2012-01-01,2020-12-31&countries=us&languages=en&runtime=5,&view=simple&sort=release_date,asc&count=250\n",
    "https://www.imdb.com/search/title/?title_type=tv_series&release_date=2012-01-01,2020-12-31&countries=us&languages=en&runtime=5,&view=simple&sort=release_date,asc&count=250&start=251&ref_=adv_nxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-distributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list for movies with missing info - FINAL VERSION FOR EXTRACTING LINKS\n",
    "error_movies = []\n",
    "#create dictionary\n",
    "data = {\n",
    "      'title': [],\n",
    "      'year': [],\n",
    "      'rating': [], 'link' : [],\n",
    "    }\n",
    "\n",
    "#counter\n",
    "x = 0\n",
    "base_url = \"https://www.imdb.com/search/title/?title_type=tv_series&release_date=2012-01-01,2020-12-31&countries=us&languages=en&runtime=5,&view=simple&sort=release_date,asc&count=250\"\n",
    "\n",
    "while x<=38:\n",
    "    if x == 0:\n",
    "        url = base_url\n",
    "    else:\n",
    "        url = base_url+\"&start={}&ref_=adv_nxt\".format(x*250 + 1)\n",
    "    response = requests.get(url)\n",
    "    response.status_code\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    show_elements=soup.find_all('div', class_='lister-item-content')\n",
    "\n",
    "    try:\n",
    "        for element in show_elements:\n",
    "        #extract title\n",
    "    \n",
    "            title = element.find('div', class_='col-title').find('a')\n",
    "            data['title'].append(title.text)\n",
    "            \n",
    "        #extract link\n",
    "            link = title['href']\n",
    "            data['link'].append(link)\n",
    "    \n",
    "        #extract year\n",
    "    \n",
    "            year_released = element.find('div', class_=\"col-title\").find('a').findNext().text\n",
    "            year_text = year_released.replace('(', '')\n",
    "            year_text = year_text.replace('â€“', '')\n",
    "            year_text = year_text.replace(')', '')\n",
    "            year_text = year_text.replace('I', '')\n",
    "            data['year'].append(year_text)\n",
    "    \n",
    "        # extract rating\n",
    "            ratings = element.find('div', class_=\"col-imdb-rating\").find('strong').text.strip()[-3:]\n",
    "            data['rating'].append(ratings)\n",
    "    except:\n",
    "        data['rating'].append('Missing')\n",
    "        error_movies.append(title.text)\n",
    "    x+=1\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "compatible-shareware",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "concrete-energy",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deadly-terror",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "entire-label",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "latter-discussion",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "found-thursday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save links to csv\n",
    "df.to_csv('file_name.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "endless-exemption",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_show_dict(link):\n",
    "    \n",
    "    inner_page = {'title': None, 'year':None, 'rating':None, 'num_reviews': None, 'genre':None,\n",
    "                  'certificate': None, 'num_episodes':None, 'actors':None, 'network':None, 'runtime':None, 'awards':None}\n",
    "\n",
    "    '''\n",
    "    From IMDB link stub, request show html, parse with BeautifulSoup, and\n",
    "    collect \n",
    "        - title \n",
    "        - number of reviews\n",
    "        - runtime \n",
    "        - genre\n",
    "        - certificate\n",
    "        - number of episodes\n",
    "        - actors\n",
    "        - network\n",
    "    Return information as a dictionary.\n",
    "    '''\n",
    "    \n",
    "    base_url = 'https://www.imdb.com/'\n",
    "    \n",
    "    #Create full url to scrape\n",
    "    url = base_url + link\n",
    "    \n",
    "    #Request HTML and parse\n",
    "    response = requests.get(url)\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page,\"lxml\")\n",
    "    \n",
    "    # get title of show\n",
    "    title = soup.find('div', class_='title-overview').find('div', class_='title_wrapper').find('h1').text.strip()\n",
    "    inner_page['title']=title\n",
    "\n",
    "    try:\n",
    "        # YEAR ON SHOW PAGE\n",
    "        year = soup.find('div', class_='title_wrapper').find('div', class_='subtext').find_all('a')[-1].text.split()[-1]\n",
    "        clean_year = year.replace(')', '').replace('(', '')\n",
    "        inner_page['year'] =clean_year\n",
    "    except:\n",
    "        inner_page['year'] ='Missing'\n",
    "        \n",
    "    try:\n",
    "        # RATING ON SHOW PAGE\n",
    "\n",
    "        rating = soup.find('div', class_='ratingValue').find('span', itemprop='ratingValue').text\n",
    "        inner_page['rating'] = rating\n",
    "    except:\n",
    "        inner_page['rating']= 'Missing'\n",
    "        \n",
    "        # number of reviews\n",
    "    try:\n",
    "        num_reviews = soup.find('span', class_='small').text\n",
    "        inner_page['num_reviews'] = num_reviews\n",
    "    except:\n",
    "        inner_page['num_reviews'] = 0\n",
    "\n",
    "    # genres & certificate\n",
    "\n",
    "    try:\n",
    "        for genre in soup.find('div', {'id':'titleStoryLine'}).find_all('div', class_='see-more inline canwrap')[1].find_all('a'):\n",
    "            clean_genre = genre.text.strip()\n",
    "            inner_page['genre'] = clean_genre\n",
    "    except:\n",
    "        inner_page['genre'] = 'Missing'\n",
    "\n",
    "    try:\n",
    "        certificate = soup.find('div', {'id':'titleStoryLine'}).find('div', class_='txt-block').find('span').text\n",
    "        inner_page['certificate'] = certificate\n",
    "    except:\n",
    "        inner_page['certificate'] = 'Missing'\n",
    "\n",
    "    # num of episodes\n",
    "\n",
    "    try:\n",
    "        episodes = soup.find('div', class_='bp_content').find('span').text.replace('episodes', '').strip()\n",
    "        inner_page['num_episodes'] = episodes\n",
    "    except:\n",
    "        inner_page['num_episodes'] = 0\n",
    "\n",
    "    #cast\n",
    "\n",
    "    try:\n",
    "        for actor in soup.find_all('div', class_=\"credit_summary_item\")[-1].find_all('a')[:-1]:\n",
    "            clean_actor = actor.text.strip()\n",
    "            inner_page['actors'] = clean_actor\n",
    "    except:\n",
    "        inner_page['actors'] = 'Missing'\n",
    "\n",
    "    #network\n",
    "\n",
    "    try:\n",
    "        for network in soup.find('div', {'id':'titleDetails'}).find_all('div', class_='txt-block')[5].find_all('a')[:-1]:\n",
    "            clean_network = network.text.strip()\n",
    "            inner_page['network'] = clean_network\n",
    "    except:\n",
    "        inner_page['network'] = 'Missing'\n",
    "\n",
    "    # runtime\n",
    "    try:\n",
    "        runtime = soup.find('div', {'id':'titleDetails'}).find('time').text\n",
    "        inner_page['runtime'] = runtime\n",
    "    except:\n",
    "        inner_page['runtime'] = 'Missing'\n",
    "\n",
    "    if soup.findAll(text=re.compile('award')):\n",
    "        inner_page['awards'] = 1\n",
    "    else:\n",
    "        inner_page['awards'] = 0\n",
    "\n",
    "    return inner_page\n",
    "\n",
    "# this works, got rid of the brackets in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "extra-resident",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating test list to make sure everything is working\n",
    "test_list = []\n",
    "\n",
    "for link in links_df[:100].link:\n",
    "    test_list.append(get_show_dict(link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "wired-committee",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ethical-brave",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(test_list)\n",
    "test_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "technical-bumper",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_set = []\n",
    "\n",
    "for link in links_df.link:\n",
    "    full_set.append(get_show_dict(link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "adequate-intersection",
   "metadata": {},
   "outputs": [],
   "source": [
    "full = pd.DataFrame(full_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "proper-characteristic",
   "metadata": {},
   "outputs": [],
   "source": [
    "full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "canadian-myrtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "tired-finder",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.merge(links_df, full, how='outer', left_on = 'title', right_on='title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-garden",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-tuition",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.drop(columns=['Unnamed: 0', 'link'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-climate",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-accused",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_csv('merged_with_links_2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
